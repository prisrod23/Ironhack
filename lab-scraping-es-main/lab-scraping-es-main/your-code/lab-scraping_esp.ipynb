{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "Encontrarás en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [código de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el código HTML en cada solicitud para entender el tipo de información que estás obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/información solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a través de Chrome DevTools. Necesitarás identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos Útiles\n",
    "- Documentación de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de códigos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos básicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos básicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu código no se pierda en la salida.**\n",
    "\n",
    "#### Desafío 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la página de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m github_html\u001b[38;5;241m=\u001b[39m\u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(github_html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Imprimir los resultados\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza técnicas de manipulación de cadenas para reemplazar espacios en blanco y saltos de línea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida debería lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mattt (mattt)', 'Andrej (karpathy)', 'Emil Ernerfeldt (emilk)', 'Gabriel Luiz Freitas Almeida (ogabrielluiz)', 'Jason Jean (FrozenPandaz)', 'Etienne Baudoux (veler)', 'Vectorized (Vectorized)', 'James Newton-King (JamesNK)', 'Mitchell Hashimoto (mitchellh)', 'Lucain (Wauplin)', 'Guillaume Dalle (gdalle)', 'Marshall Bowers (maxdeviant)', 'Alex Gaynor (alex)', 'Jimmi Dyson (jimmidyson)', 'LangChain4j (langchain4j)', 'Nathan Flurry (NathanFlurry)', 'Henrik Rydgård (hrydgard)', 'Deshraj Yadav (deshraj)', 'Mateusz \"Serafin\" Gajewski (wendigo)', 'Eric Buehler (EricLBuehler)', 'Brad Campbell (bradjc)', 'Vik Paruchuri (VikParuchuri)', 'Costa Huang (vwxyzjn)']\n"
     ]
    }
   ],
   "source": [
    "devs=[]\n",
    "\n",
    "# Buscar los divs con la clase específica\n",
    "divs = soup.find_all('div', class_='col-md-6')\n",
    "\n",
    "# Extraer y mostrar los nombres y usernames\n",
    "for div in divs:\n",
    "    h1 = div.find('h1', class_='h3 lh-condensed')\n",
    "    p = div.find('p', class_='f4 text-normal mb-1')\n",
    "    \n",
    "    if h1 and p:\n",
    "        nombre_tag = h1.find('a')\n",
    "        username_tag = p.find('a')\n",
    "        \n",
    "        if nombre_tag and username_tag:\n",
    "            nombre = nombre_tag.get_text(strip=True)\n",
    "            username = username_tag.get_text(strip=True)\n",
    "            devs.append(f\"{nombre} ({username})\")\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mem0ai /mem0 (The memory layer for Personalized AI)', 'hiyouga /LLaMA-Factory (A WebUI for Efficient Fine-Tuning of 100+ LLMs (ACL 2024))', 'MervinPraison /PraisonAI (PraisonAI application combines AutoGen and CrewAI or similar frameworks into a low-code solution for building and managing multi-agent LLM systems, focusing on simplicity, customisation, and efficient human-agent collaboration.)', 'prowler-cloud /prowler (Prowler is an Open Source Security tool for AWS, Azure, GCP and Kubernetes to do security assessments, audits, incident response, compliance, continuous monitoring, hardening and forensics readiness. Includes CIS, NIST 800, NIST CSF, CISA, FedRAMP, PCI-DSS, GDPR, HIPAA, FFIEC, SOC2, GXP, Well-Architected Security, ENS and more)', 'modelscope /FunASR (A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.)', 'getsentry /sentry (Developer-first error tracking and performance monitoring)', 'openvinotoolkit /anomalib (An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.)', 'wenet-e2e /wenet (Production First and Production Ready End-to-End Speech Recognition Toolkit)', 'run-llama /llama_index (LlamaIndex is a data framework for your LLM applications)', 'facebookresearch /detr (End-to-End Object Detection with Transformers)', 'microsoft /promptflow (Build high-quality LLM apps - from prototyping, testing to production deployment and monitoring.)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lista para almacenar los repositorios\n",
    "repos = []\n",
    "\n",
    "# Buscar los artículos con la clase específica\n",
    "articles = soup2.find_all('article', class_='Box-row')\n",
    "\n",
    "for article in articles:\n",
    "    h2 = article.find('h2', class_='h3 lh-condensed')\n",
    "    p = article.find('p', class_='col-9 color-fg-muted my-1 pr-4')\n",
    "    \n",
    "    if h2 and p:\n",
    "        nombre_tag = h2.find('a')\n",
    "        \n",
    "        if nombre_tag:\n",
    "            nombre = nombre_tag.get_text(strip=True)\n",
    "            descripcion = p.get_text(strip=True)\n",
    "            repos.append(f\"{nombre} ({descripcion})\")\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(repos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 3 - Mostrar todos los enlaces de imágenes de la página de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/File:Walt_Disney_1946.JPG\n",
      "https://en.wikipedia.org/wiki/File:Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg\n",
      "https://en.wikipedia.org/wiki/File:Walt_Disney_envelope_ca._1921.jpg\n",
      "https://en.wikipedia.org/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg\n",
      "https://en.wikipedia.org/wiki/File:Disney_drawing_goofy.jpg\n",
      "https://en.wikipedia.org/wiki/File:WaltDisneyplansDisneylandDec1954.jpg\n",
      "https://en.wikipedia.org/wiki/File:Walt_disney_portrait_right.jpg\n",
      "https://en.wikipedia.org/wiki/File:Walt_Disney_Grave.JPG\n",
      "https://en.wikipedia.org/wiki/File:DisneySchiphol1951.jpg\n",
      "https://en.wikipedia.org/wiki/File:Disney1968.jpg\n",
      "https://en.wikipedia.org/wiki/File:Disney_Oscar_1953_(cropped).jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Buscar todas las etiquetas <img>\n",
    "img_tags = soup3.find_all('img')\n",
    "\n",
    "# Buscar el contenedor específico que contiene el contenido principal\n",
    "content_container = soup3.find('div', class_='mw-content-container')\n",
    "\n",
    "if content_container:\n",
    "    # Buscar todas las etiquetas <a> con la clase mw-file-description dentro del contenedor\n",
    "    file_description_links = content_container.find_all('a', class_='mw-file-description')\n",
    "    \n",
    "    # Lista para almacenar las URLs de las imágenes de wikipedia/commons\n",
    "    image_urls = []\n",
    "    \n",
    "    # Iterar sobre los enlaces encontrados\n",
    "    for link in file_description_links:\n",
    "        href = link.get('href')\n",
    "        #filtraje de imagenes que no sean de la wiki e iconos.\n",
    "        if href and '/wiki/File:' in href and not href.endswith('.svg'):\n",
    "            # reconstruir la URL completa \n",
    "            full_url = f\"https://en.wikipedia.org{href}\"\n",
    "            image_urls.append(full_url)\n",
    "    for url in image_urls:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No se encontró el contenedor mw-content-container en la página.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 4 - Recuperar todos los enlaces a páginas en Wikipedia que se refieren a algún tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonidae : https://en.wikipedia.org/wiki/Pythonidae\n",
      "Python (genus) : https://en.wikipedia.org/wiki/Python_(genus)\n",
      "Python (mythology) : https://en.wikipedia.org/wiki/Python_(mythology)\n",
      "Python (programming language) : https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "Python of Aenus : https://en.wikipedia.org/wiki/Python_of_Aenus\n",
      "Python (painter) : https://en.wikipedia.org/wiki/Python_(painter)\n",
      "Python of Byzantium : https://en.wikipedia.org/wiki/Python_of_Byzantium\n",
      "Python of Catana : https://en.wikipedia.org/wiki/Python_of_Catana\n",
      "Python Anghelo : https://en.wikipedia.org/wiki/Python_Anghelo\n",
      "Python (Efteling) : https://en.wikipedia.org/wiki/Python_(Efteling)\n",
      "Python (Busch Gardens Tampa Bay) : https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "Python (Coney Island, Cincinnati, Ohio) : https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "Python (automobile maker) : https://en.wikipedia.org/wiki/Python_(automobile_maker)\n",
      "Python (Ford prototype) : https://en.wikipedia.org/wiki/Python_(Ford_prototype)\n",
      "Python (missile) : https://en.wikipedia.org/wiki/Python_(missile)\n",
      "Python (nuclear primary) : https://en.wikipedia.org/wiki/Python_(nuclear_primary)\n",
      "Colt Python : https://en.wikipedia.org/wiki/Colt_Python\n",
      "Python (codename) : https://en.wikipedia.org/wiki/Python_(codename)\n",
      "Python (film) : https://en.wikipedia.org/wiki/Python_(film)\n",
      "Monty Python : https://en.wikipedia.org/wiki/Monty_Python\n",
      "Python (Monty) Pictures : https://en.wikipedia.org/wiki/Python_(Monty)_Pictures\n"
     ]
    }
   ],
   "source": [
    "# Encontrar el contenedor específico que contiene el contenido principal\n",
    "content_container = soup4.find('div', class_='mw-content-container')\n",
    "\n",
    "if content_container:\n",
    "    # Buscar todos los enlaces <a> dentro del contenedor de contenido principal\n",
    "    python_links = content_container.find_all('a', title=lambda value: value and 'Python' in value, href=lambda value: value and value.startswith('/wiki/') and '://' not in value)\n",
    "\n",
    "    # Imprimir los enlaces encontrados\n",
    "    for link in python_links:\n",
    "        # Reconstruir la URL completa\n",
    "        full_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "        print(link['title'], ':', full_url)\n",
    "else:\n",
    "    print(\"No se encontró el contenedor mw-content-container en la página.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 5 - Número de Títulos que han cambiado en el Código de los Estados Unidos desde su último punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'http://uscode.house.gov/download/download.shtml'\n",
    "python1 = requests.get(f\"{url5}\").text\n",
    "soup5 = BeautifulSoup(python1, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "titles_changes = soup5.find_all('div',class_='usctitlechanged')\n",
    "changes = []\n",
    "for title in titles_changes:\n",
    "    changes.append(title.get_text(strip= True))\n",
    "print(len(changes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 6 - Una lista de Python con los diez nombres más buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.fbi.gov/wanted/topten'\n",
    "r= requests.get(url7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(wanted, 'html.parser')\n",
    "wantedtag = soup7.find_all('h3', class_='title')\n",
    "wanted = []\n",
    "for want in wantedtag:\n",
    "    wanted.append(title.get_text(strip= True))\n",
    "\n",
    "print(wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 7 - Listar todos los nombres de idiomas y el número de artículos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\").text\n",
    "soup8 = BeautifulSoup(languages, 'html.parser')\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['English', '6,847,000+ articles'],\n",
       " ['æ\\x97¥æ\\x9c¬èª\\x9e', '1,421,000+ è¨\\x98äº\\x8b'],\n",
       " ['Deutsch', '2.924.000+ Artikel'],\n",
       " ['Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹', '1Â\\xa0987Â\\xa0000+ Ñ\\x81Ñ\\x82Ð°Ñ\\x82ÐµÐ¹'],\n",
       " ['EspaÃ±ol', '1.965.000+ artÃ\\xadculos'],\n",
       " ['FranÃ§ais', '2â\\x80¯621â\\x80¯000+ articles'],\n",
       " ['ä¸\\xadæ\\x96\\x87', '1,429,000+ æ\\x9d¡ç\\x9b® / æ¢\\x9dç\\x9b®'],\n",
       " ['Italiano', '1.871.000+ voci'],\n",
       " ['Ù\\x81Ø§Ø±Ø³Û\\x8c', 'Û±Ù¬Û°Û°Û¶Ù¬Û°Û°Û°+ Ù\\x85Ù\\x82Ø§Ù\\x84Ù\\x87'],\n",
       " ['PortuguÃªs', '1.128.000+ artigos']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idiomas = []\n",
    "clean_language=[]\n",
    "for idioma in langlist:\n",
    "    idiomas.append(idioma.get_text().strip())\n",
    "for elem in idiomas:\n",
    "    clean_language.append(elem.split(\"\\n\"))\n",
    "clean_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_uk = soup8.find_all(\"h3\", class_=\"govuk-heading-s dgu-topics__heading\")\n",
    "len(info_uk)\n",
    "clean_uk=[]\n",
    "for elem in info_uk:\n",
    "    clean_uk.append(elem.get_text(strip=True))\n",
    "clean_uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 9 - Los 10 idiomas con más hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>native_speakers</th>\n",
       "      <th>language_family</th>\n",
       "      <th>branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>941</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>486</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>380</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>345</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>237</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>236</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>148</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>123</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>86</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>85</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           language native_speakers language_family        branch\n",
       "0  Mandarin Chinese             941    Sino-Tibetan       Sinitic\n",
       "1           Spanish             486   Indo-European       Romance\n",
       "2           English             380   Indo-European      Germanic\n",
       "3             Hindi             345   Indo-European    Indo-Aryan\n",
       "4           Bengali             237   Indo-European    Indo-Aryan\n",
       "5        Portuguese             236   Indo-European       Romance\n",
       "6           Russian             148   Indo-European  Balto-Slavic\n",
       "7          Japanese             123         Japonic      Japanese\n",
       "8       Yue Chinese              86    Sino-Tibetan       Sinitic\n",
       "9        Vietnamese              85   Austroasiatic        Vietic"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla = soup9.find('table')\n",
    "if tabla is None:\n",
    "    print(\"No se encuentra la tabla\")\n",
    "    exit()\n",
    "tabla\n",
    "\n",
    "lista_de_idiomas=[]\n",
    "filas = tabla.find_all('tr')[1:]\n",
    "for fila in filas:\n",
    "    celdas = fila.find_all('td')\n",
    "    language= celdas[0].text.strip()\n",
    "    native_speakers= celdas[1].text.strip()\n",
    "    language_family= celdas[2].text.strip()\n",
    "    branch=celdas[3].text.strip()\n",
    "    dic_idioma={\n",
    "        \"language\":language,\n",
    "        \"native_speakers\": native_speakers,\n",
    "        \"language_family\": language_family,\n",
    "        \"branch\": branch\n",
    "    }\n",
    "    lista_de_idiomas.append(dic_idioma)\n",
    "\n",
    "df_idiomas= pd.DataFrame(lista_de_idiomas[:10])\n",
    "df_idiomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "#### Desafío 10 - La información de los 20 últimos terremotos (fecha, hora, latitud, longitud y nombre de la región) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "url7 = \"https://www.emsc-csem.org/#2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not working :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 11 - Datos del Top 250 de IMDB (nombre de la película, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head><title>403 Forbidden</title></head>\n",
      "<body>\n",
      "<center><h1>403 Forbidden</h1></center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top'\n",
    "pelis = requests.get(url11)\n",
    "soup = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 1. Cadena perpetua, Link: https://www.imdb.com/title/tt0111161/?ref_=chttp_t_1\n",
      "Title: 2. El padrino, Link: https://www.imdb.com/title/tt0068646/?ref_=chttp_t_2\n",
      "Title: 3. El caballero oscuro, Link: https://www.imdb.com/title/tt0468569/?ref_=chttp_t_3\n",
      "Title: 4. El padrino parte II, Link: https://www.imdb.com/title/tt0071562/?ref_=chttp_t_4\n",
      "Title: 5. 12 hombres sin piedad, Link: https://www.imdb.com/title/tt0050083/?ref_=chttp_t_5\n",
      "Title: 6. La lista de Schindler, Link: https://www.imdb.com/title/tt0108052/?ref_=chttp_t_6\n",
      "Title: 7. El señor de los anillos: El retorno del rey, Link: https://www.imdb.com/title/tt0167260/?ref_=chttp_t_7\n",
      "Title: 8. Pulp Fiction, Link: https://www.imdb.com/title/tt0110912/?ref_=chttp_t_8\n",
      "Title: 9. El señor de los anillos: La comunidad del anillo, Link: https://www.imdb.com/title/tt0120737/?ref_=chttp_t_9\n",
      "Title: 10. El bueno, el feo y el malo, Link: https://www.imdb.com/title/tt0060196/?ref_=chttp_t_10\n",
      "Title: 11. Forrest Gump, Link: https://www.imdb.com/title/tt0109830/?ref_=chttp_t_11\n",
      "Title: 12. El señor de los anillos: Las dos torres, Link: https://www.imdb.com/title/tt0167261/?ref_=chttp_t_12\n",
      "Title: 13. El club de la lucha, Link: https://www.imdb.com/title/tt0137523/?ref_=chttp_t_13\n",
      "Title: 14. Origen, Link: https://www.imdb.com/title/tt1375666/?ref_=chttp_t_14\n",
      "Title: 15. El imperio contraataca, Link: https://www.imdb.com/title/tt0080684/?ref_=chttp_t_15\n",
      "Title: 16. Matrix, Link: https://www.imdb.com/title/tt0133093/?ref_=chttp_t_16\n",
      "Title: 17. Uno de los nuestros, Link: https://www.imdb.com/title/tt0099685/?ref_=chttp_t_17\n",
      "Title: 18. Alguien voló sobre el nido del cuco, Link: https://www.imdb.com/title/tt0073486/?ref_=chttp_t_18\n",
      "Title: 19. Interstellar, Link: https://www.imdb.com/title/tt0816692/?ref_=chttp_t_19\n",
      "Title: 20. Seven, Link: https://www.imdb.com/title/tt0114369/?ref_=chttp_t_20\n",
      "Title: 21. ¡Qué bello es vivir!, Link: https://www.imdb.com/title/tt0038650/?ref_=chttp_t_21\n",
      "Title: 22. Los siete samuráis, Link: https://www.imdb.com/title/tt0047478/?ref_=chttp_t_22\n",
      "Title: 23. El silencio de los corderos, Link: https://www.imdb.com/title/tt0102926/?ref_=chttp_t_23\n",
      "Title: 24. Salvar al soldado Ryan, Link: https://www.imdb.com/title/tt0120815/?ref_=chttp_t_24\n",
      "Title: 25. Ciudad de Dios, Link: https://www.imdb.com/title/tt0317248/?ref_=chttp_t_25\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the IMDb Top 250 movies page\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "\n",
    "# Set the headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Print a portion of the parsed HTML to check the structure\n",
    "    # print(soup.prettify()[:1000])  # Print the first 1000 characters for inspection\n",
    "\n",
    "    # Find the movie containers\n",
    "    movie_containers = soup.find_all('div', class_='ipc-title ipc-title--base ipc-title--title ipc-title-link-no-icon ipc-title--on-textPrimary sc-b189961a-9 bnSrml cli-title')\n",
    "\n",
    "    # Initialize an empty list to store movie details\n",
    "    movies = []\n",
    "\n",
    "    # Loop through all movie containers and extract movie details\n",
    "    for container in movie_containers:\n",
    "        # Extract the title text\n",
    "        title = container.find('h3', class_='ipc-title__text').text\n",
    "\n",
    "        # Extract the link\n",
    "        link = container.find('a', class_='ipc-title-link-wrapper')['href']\n",
    "\n",
    "        # Append the movie details to the list\n",
    "        movies.append({'title': title, 'link': 'https://www.imdb.com' + link})\n",
    "\n",
    "    # Print the list of movies\n",
    "    for movie in movies:\n",
    "        print(f\"Title: {movie['title']}, Link: {movie['link']}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 12 - Nombre de la película, año y un breve resumen de las 10 películas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(soup.find_all(\"span\", {\"class\": \"secondaryInfo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 13 - Encontrar el reporte meteorológico en vivo (temperatura, velocidad del viento, descripción y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "API_key = \"037dee7fd30dd7050c094934ea4d7807\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ciudad': 'buffalo',\n",
       " 'Temperatura': 294.92,\n",
       " 'Velocidad del viento': 2.06,\n",
       " 'Descripcion': 'broken clouds',\n",
       " 'Clima': 'Clouds'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weather(city):\n",
    "    api_weather = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_key}\"\n",
    "    weath = requests.get(api_weather)\n",
    "    if weath.status_code == 200:\n",
    "        data_w = weath.json()\n",
    "        \n",
    "    else:\n",
    "         print(f\"Error en la solicitud: {weath.status_code}\")\n",
    "    clima ={ \"Ciudad\": city , \"Temperatura\":data_w[\"main\"][\"temp\"], \"Velocidad del viento\": data_w[\"wind\"][\"speed\"] , \"Descripcion\": data_w[\"weather\"][0][\"description\"] , \"Clima\": data_w[\"weather\"][0][\"main\"]}\n",
    "    return clima\n",
    "   \n",
    "\n",
    "weather('buffalo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url14)\n",
    "soup = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title   Price Availability\n",
       "0                      A Light in the ...  £51.77     In stock\n",
       "1                      Tipping the Velvet  £53.74     In stock\n",
       "2                              Soumission  £50.10     In stock\n",
       "3                           Sharp Objects  £47.82     In stock\n",
       "4            Sapiens: A Brief History ...  £54.23     In stock\n",
       "5                         The Requiem Red  £22.65     In stock\n",
       "6            The Dirty Little Secrets ...  £33.34     In stock\n",
       "7                 The Coming Woman: A ...  £17.93     In stock\n",
       "8                     The Boys in the ...  £22.60     In stock\n",
       "9                         The Black Maria  £52.15     In stock\n",
       "10  Starving Hearts (Triangular Trade ...  £13.99     In stock\n",
       "11                  Shakespeare's Sonnets  £20.66     In stock\n",
       "12                            Set Me Free  £17.46     In stock\n",
       "13    Scott Pilgrim's Precious Little ...  £52.29     In stock\n",
       "14                      Rip it Up and ...  £35.02     In stock\n",
       "15                  Our Band Could Be ...  £57.25     In stock\n",
       "16                                   Olio  £23.88     In stock\n",
       "17        Mesaerion: The Best Science ...  £37.59     In stock\n",
       "18           Libertarianism for Beginners  £51.33     In stock\n",
       "19                It's Only the Himalayas  £45.17     In stock"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "book_list=[]\n",
    "\n",
    "for item in catalog:\n",
    "    #extraccion titulos\n",
    "    title=item.find(\"h3\").get_text(strip=True)\n",
    "    #extraccion precio\n",
    "    price = item.find(\"p\", class_=\"price_color\").get_text(strip=True)\n",
    "    #extraccion stock\n",
    "    availability= item.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
    "    info = {\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Availability\": availability\n",
    "    }\n",
    "    book_list.append(info)\n",
    "\n",
    "finalbooks= pd.DataFrame(book_list)\n",
    "finalbooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
